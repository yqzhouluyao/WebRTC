### 1、什么是WebRTC?

WebRTC是一些标准协议和API的集合，

它能够在没有插件和软件的前提下，使得浏览器和浏览器之间、浏览器和Native App之间实时通讯，

包括实时视频、实时音频、和数据分享。



### 2、WebRTC的主要组件是什么？

有3个主要组件。

getUserMedia API，允许访问用户的摄像头和麦克风

RTCPeerConnection API，管理不同的浏览器之间的连接，能够实时通讯

RTCDataChannel API，允许在端与端之间交换任意数据



### 3、WebRTC是怎么处理NAT穿越的？

WebRTC用多种技术保证NAT穿越的，包括STUN (Session Traversal Utilities for NAT), TURN (Traversal Using Relay NAT), and ICE (Interactive Connectivity Establishment)，

 这些协议能够帮助识别在NAT后设备的公网IP地址，尽管这些设备可能出在不同的NAT类型，依然能够拿到公网IP地址建立连接。



### 4、WebRTC有哪些应用？

视频会议、直播推拉流、在线游戏、文件分享、以及其他实时通讯的应用

##### 在线游戏有哪些应用方式呢？

音频通话、视频通话，WebRTC可以直接集成到游戏中，而不需要接入第三方的通讯服务。

2、低延迟数据传输：同步游戏状态，发送游戏指令

3、WebRTC能够减少延迟提高游戏的整体表现，能够直接点对点通讯而不用借助中心服务器，结果就是能够更快响应，游戏体验更好。





### 5、WebRTC是如何确保安全和隐私的？

首先两个端之间实时通讯采用的是端到端的加密，

在访问摄像头、麦克风权限的时候WebRTC内置了权限访问提示，

网络传输是独立的，避免了第三方服务的追踪



### 6、WebRTC是如何实现可靠传输的？它使用的是什么协议

WebRTC使用了UDP和TCP协议，具体取决于在哪应用，

TCP是可靠传输协议，保证发送是数据是有序的，包括错误检测和错误纠正机制，

TCP的典型的应用在非实时数据传输的情况，比如文件的下载，应用在那些可靠性和错误纠正比传输速度重要的场景，

UDP是轻量级的传输协议，它的传输不具有可靠性和错误纠正机制，应用在对延迟比较低，不要求每个包都到达的场景，比如音视频通讯，在线游戏等

RTCPeerConnection 就即用了UDP协议又用了TCP协议，

在信令初始化阶段，两个端交换网络地址使用信令通讯就是使用WebSocket结合TCP实现的，

一旦两端建立连接，就切换成UDP协议传输数据保证最小延迟，确保实时响应



##### WebRTC为了保证UDP的可靠传输做了哪些处理？

检测数据错误的包和丢包恢复，

拥塞控制，会根据网络状况是控制发送数据包的速度，像一些丢包恢复和重传机制确保丢的包能够尽快实现重传，



### 7、在WebRTC中丢包检测是如何实现的？

主要用到了3个技术：

1、唯一序列号

每个包通过数据通道发送都会赋一个唯一序列号，接收者受到序列号的时候会和自己期望的序列号做比较，如果是乱序的，就说明有一些包丢了，然后请求重传那些丢失的数据包



2、定时机制

WebRTC检测数据包到达的时间确定一个包是否丢失，如果接收者在一个确定的时间窗口没有收到数据包，就认定这个包丢了，请求重传



3、RTCP 反馈机制

3.1、 WebRTC会用RTCP的receiver reports (RR) 和 sender reports (SR) 去提供丢包的反馈

3.2 、RTT(round-trip time )指标，就是一个包到对端，然后再回来需要的时间，它受网络拥塞，两端距离，和处理数据包的时间影响

3.3 、抖动指标， 两个数据包传输延迟的变化，它测量的是数据包在两个端点传输时间的不一致性，网络拥塞、路由变化、和不同包的处理时间都会引起抖动，高级别的网络抖动会导致视频终端，延迟，甚至通讯中断。



总之，丢包重传重点在于重传机制和根据网络拥塞情况调整数据包的发送速度。





### 8、WebRTC是如何实现音视频同步的？

##### 1、采集包打时间戳的方式

主要的技术是用RTP (Real-time Transport Protocol)协议，它在WebRTC中用于传输音视频包，

RTP把刚采集到的音视频包用一个时间戳标记，这个时间戳用于接收者去同步音视频流，

接收者在相应的播放时间把音视频包进行对齐，确保同步。



##### 2、WebRTC还使用了一种自适应抖动缓冲技术

根据网络的延迟调整数据包的时间，根据网络的抖动量调整音频、视频数据的播放，

有助于网络延迟发生时也能保证音视频同步。



##### 实现思路：

1、向Media Stream中添加音频轨和视频轨

2、将流添加到PeerConnection中进行同步

3、为每一个视频帧计算相应的音频时间戳，并在渲染



##### 具体的代码实现逻辑:

获取音频时钟、视频时钟，然后把对应的数据解码出来，如果视频时钟 大于 音频时钟 ，

意味着视频包被过早的解码出来，那就需要等待对应的音频时钟到达，就休眠等待5ms，再次检测，

SDL的调用解码音频的频率已经定好了，以音频为标准，视频要同步到音频，

每拿出一个音频包，我就拿出来看下时间是什么，把音频时钟保存下来

在视频解码的地方也会拿出一个包，拿出视频时钟，先算出视频包应该在什么时候显示，如果解码出来，像素格式转换完了，不是马上就显示出来

而是先看一下视频包的时钟是不是大于音频包的时钟，如果大于需要赶紧显示出来，否则需要休眠等待，音频包的到来

 

  







